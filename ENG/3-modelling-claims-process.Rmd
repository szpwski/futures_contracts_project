---
title: "Gamma distribution modeling of insurance claim volume."
author: "Szymon Pawłowski"
output:
  html_document:
    df_print: paged
---

```{r claims-setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Modeling the size of insurance claims by Gamma distribution

## Introduction

This paper will briefly present a theoretical approach to the topic of generating numbers from the Gamma distribution, and then two generators from this distribution will be implemented. The first will be a generator created using the GKM1 algorithm, while the second will be based on the Ahrens-Dieter algorithm. The times, as well as the average iteration numbers from both implementations will be compared. 

The Poisson distribution number generator created in the last project will also be cited, with a slight modification to exclude the use of an in-house uniform distribution number generator due to the use of a built-in generator. Later, the insurance claims process will be modeled using the aforementioned generator, as well as the implemented generator of numbers from the Gamma distribution. 

## Gamma distribution
Gamma distribution with a shape parameter $a$ and a scale $beta$ has a mensuration equal to 
$$f(y) = f_{a, \beta}(y) = \frac{1}{\Gamma (a)\beta^a}y^{a-1}e^{-y/\beta}, \quad y\geq 0$$
Such a distribution has a mean equation of $a\beta$ and a variance of $a\beta^2$. We can distinguish sampling methods from the Gamma distribution into those that consider the situation for $a \leq 1$ and $a > 1$. When using the generator to model interest rates, in most cases we will encounter the situation with $a>1$, while the situation for $a \leq 1$ will also be considered. On the other hand, there is no loss of generality when assuming $a = 1$, because if $X$ has a Gamma distribution with parameters $(a,1)$ then $a X$ has a Gamma distribution with parameters $(a, \beta)$.

We will also create a function to plot the density.

```{r claims-r2, echo=TRUE}
plot_hist <- function(X){
  if( typeof(X) == "list"){
    S <- unlist(X[1])
    iters <- unlist(X[2])
    title = "Histogram X - implemented generator"
    txt = "Generator implemented"
    print(paste("Number of iterations: ",iters, sep=""))
  } else {
    S <- X
    title = "Histogram X - built-in generator"
    txt = "Built-in generator"
  }
  hist(S, prob=TRUE, col="grey", xlab = "X", main = title)
  lines(density(S), col="blue", lwd=2)
  print(paste(txt, " average: ", mean(S), sep=""))
  print(paste(txt, " variance: ", var(S), sep=""))
  
}
```

## GKM1 algorithm.
Suppose that some function $f$ is non-negative and integral on the set $[0, \infty)$. If $(X,Y)$ has a uniform distribution on the set $A = \{(x,y): x \leq \sqrt{f(y/x)}\}$ then the density of $Y/X$ is proportional to $f$. Thus, to uniformly sample from $A$ we can recursively, uniformly select pairs $(X,Y)$ from the rectangle and keep the first ones for which $X \leq \sqrt{f(Y/X)}$.
In order to sample from the Gamma density with parameter $a > 1$, we will define a set $A$, viz.
$$A = \{(x,y): 0\leq x \leq \sqrt{[(y/x)^{a-1}e^{-y/x}}\}$$
This set is contained in the rectangle $[0,\overline{x}]\times [0,\overline{y}]$ where $\overline{x} = [(a-1)/e]^{(a-1)/2}$. Sampling uniformly from this rectangle, the expected number of samples needed to time the dropout in $A$ is determined by the ratio of the dimension of $A$ to the dimension of this rectangle. It is defined by $O(\sqrt{a})$, so the time needed to generate a sample with this method increases with the shape parameter. In the GKM1 algorithm, we have an acceptance test performed first, which reduces the number of logarithmic transformations. Below is the implemented algorithm.

```{r claims-1r, echo=TRUE}
gkm1 <- function(a, beta, N){
  if(a <= 1){
    return("Invalid parameter a!")
  } else {
    ap <- a-1
    b <- (a-(1/(6*a)))/ap
    m <- 2/ap
    d <- m+2
    
    niter <- 0 
    X <- c()
    
    while(length(X) < N){
      sample <- "Reject"
      while(sample != "Accept"){
        niter <- niter + 1
        U1 <- runif(1)
        U2 <- runif(1)
        V <- b*U2/U1
      
        check1 <- m*U1 - d + V + (1/V)
        check2 <- m*log(U1) - log(V) + V - 1

        if(check1 <= 0){
          sample <- "Accept"
        } else if(check2 <= 0){
          sample <- "Accept"
        } else {
          sample <- "Reject"
        }
      }
    X <- append(X, beta*ap*V)
    }
  }
  
  return(list(X, niter))
}
```

## Ahrens-Dieter algorithm.
The Ahrens-Dieter algorithm handles the case when $a \leq 1$. It generates "candidates" by sampling from distributions centered on the intervals $[0,1]$ and $(1, \infty)$ with appropriate probabilities. In particular, let $p = e/(a + e)$ and
$$g(z) = \left\{ \begin{array}{ll} paz^{a-1}, &  0 \leq z \leq 0\\
        (1-p)e^{-z+1}, & z > 1.\end{array} \right. $$
This is a density that is a "mixture" of the densities $az^{a-1}$ on $[0,1]$ and $e^{-z+1}$ on $(1,\infty)$ with corresponding weights $p$ and $(1-p)$. We can sample from $g$ by sampling from each of these densities with an assigned probability. Each of these two densities is easily sampled using the inverse transform method, i.e., for the density $az^{a-1}$ we can use $U^{1/a}$, where $U \sim Unif[0,1]$ and for the density $e^{-z+1}$ we have $1-log(U)$. Samples with $g$ are suitable candidates for the accept-rejection method because the ratio $f_{a,1}(z)/g(z)$ with $f_{a,1}$ Gamma density is limited. Insight into this ratio reveals that candidate $Z$ on the interval $[0,1]$ is accepted with probability $e^{-Z}$, and candidate $(1, \infty)$ is accepted with probability $Z^{a-1}$. The global ratio constraint is given by
$$f_{a,1}(z) / g(z) \leq \frac{a+e}{ae\Gamma(a)} \leq 1.39,$$
where it is worth recalling that the upper bound on the restriction of such a proportion determines the expected number of candidates generated with the accepted sample. In the following algorithm, it is worth noting that when the condition $Yleq 1$ fails then $Y$ has a uniform distribution on $[1,b]$ which means that $(b-Y)/a$ has a distribution $U/e$ ($U \sim Unif[0,1]$), and thus $-log((b-Y)/a)$ has a distribution $1-log(U)$. 

```{r claims-rar, echo=TRUE}
adr <- function(a, beta, N){
  if(a > 1){
    return("Invalid parameter a!")
  } else {
    b <- (a + exp(1))/exp(1)
    
    niter <- 0 
    X <- c()
    
    while(length(X) < N){
      sample <- "Reject"
      while(sample != "Accept"){
        niter <- niter + 1
        U1 <- runif(1)
        U2 <- runif(1)
        Y <- b*U1
        
        if (Y <= 1){
          Z <- Y^(1/a)
          if (U2 < exp(-Z)){
            sample <- "Accept"
          }
        } else {
          Z <- -log((b-Y)/a)
          if (U2 <= Z^(a-1)){
            sample <- "Accept"
          }
        }
      }
    X <- append(X, beta*Z)
    }
  }
  
  return(list(X, niter))
}
```

## Testing
After implementing both algorithms, we will prepare a function that will allow us to freely use the generator of numbers from the Gamma distribution depending on the assumed shape parameter $a$.

```{r claims-rgamma, echo=TRUE}
gam <- function(a, beta, N, unlist=FALSE){
  if (a <= 1){
    G <- adr(a,beta,N)
  } else {
    G <- gkm1(a, beta, N)
  }
  
  if (unlist){
    G <- unlist(G[1])
  }
  return(G)
}
```

Let's start with the case of $a <= 1$ and plot the histogram with the adjacency to the sample generated from the embedded generator. Let $a = 1$ and $a = 2$ for a sample of $N=1000$. 

```{r claims-r3comp, echo=TRUE}
G1 <- gam(1,2,1000)
G2 <- rgamma(n=1000, scale=2, shape=1)
plot_hist(G1)
plot_hist(G2)
```

We can easily see the convergence of the two samples. Let's check, then, the situation for the parameters $a = 0.5$ and $\beta = 1$.

```{r claims-r3comp2, echo=TRUE}
G1 <- gam(0.5,1,1000)
G2 <- rgamma(n=1000, scale=1, shape=0.5)
plot_hist(G1)
plot_hist(G2)
```

Again, the histograms are close to each other. This demonstrates the correctness of the implemented algorithm and the achievement of correct sample results for the case $a<=1$. Moreover, it shows the actual lack of loss of generality when assuming $a=1$ during implementation and multiplying by this factor in the final stage. Let's see the situation for $a>1$, so let's consider $a = 2$ and $\beta = 2$. 

```{r claims-r3comp3, echo=TRUE}
G1 <- gam(2,2,1000)
G2 <- rgamma(n=1000, scale=2, shape=2)
plot_hist(G1)
plot_hist(G2)
```

Again, as expected, we also get convergence of the two histograms. Let's consider some more 
$a = 9$ and $beta = 0.5$.

```{r claims-r3comp4, echo=TRUE}
G1 <- gam(9,0.5,1000)
G2 <- rgamma(n=1000, scale=0.5, shape=9)
plot_hist(G1)
plot_hist(G2)
```

Indeed, the values are still close. Moreover, in addition to the visual comparison, we can see that despite small deviations, the values of the statistics (mean and variance) are close to each other. We conclude from this that the implemented algorithm for generating a sample from the gamma distribution is correct.

The next thing we will check is the time and number of iterations needed to generate a sample depending on the assumed parameter $a$, as we then use different algorithms. Let's first check for $N \in\{100, 1000, 5000, 10000, 20000\}$ with $a = 0.5$ as $a <=1$ (so Ahrens-Dieter algorithm) and $a=2$ as $a>1$ (GKM1 algorithm).

```{r claims-r2time2, echo=TRUE}
iters1 <- c()
time1 <- c()
iters2 <- c()
time2 <- c()
x <- c()
N <- c()
for (n in c(100, 1000, 5000, 10000, 20000)){
  N <- append(N, n)

  start.time <- as.numeric(as.numeric(Sys.time())*1000, digits=15)
  G1 <- gam(0.5, 2, N=n)
  end.time <-  as.numeric(as.numeric(Sys.time())*1000, digits=15)
  t1 <- end.time - start.time


  iters1 <- append(iters1, unlist(G1[2]))
  time1 <- append(time1, t1)
  
  start.time <- as.numeric(as.numeric(Sys.time())*1000, digits=15)
  G2 <- gam(2, 2, N=n)
  end.time <-  as.numeric(as.numeric(Sys.time())*1000, digits=15)
  t2 <- end.time - start.time

  
  iters2 <- append(iters2, unlist(G2[2]))
  time2 <- append(time2, t2)
  
  print(paste("Number of iterations for a <=1: ", unlist(G1[2]), " with N=", n,sep=""))
  print(paste("Number of iterations for a >1: ", unlist(G2[2]), " with N=", n, sep=""))
  
  print(paste("The execution time of the algorithm for a <=1: ", t1, " with N=", n,sep=""))
  print(paste("The execution time of the algorithm for a >1: ", t2, " with N=", n, sep=""))

}

plot(N, iters1,  type = "b", col = "red" , lwd = 3, pch = 1, xlab="", ylab="")
lines(N, iters2, type = "b", col = "green" , lwd = 3, pch = 1)
title(main = "Number of iterations relative to sample size", xlab = "Sample size N", ylab="Number of iterations")
legend("topleft", legend=c("Ahrens-Dieter (a<=1)", "GKM1 (a>1)"), col=c("red","green"),lty=1:2, cex=0.8)

plot(N, time1,  type = "b", col = "red" , lwd = 3, pch = 1, xlab="", ylab="")
lines(N, time2, type = "b", col = "green" , lwd = 3, pch = 1)
title(main = "Number of iterations relative to sample size", xlab = "Wielkość próbki N", ylab="Czas [ms]")
legend("topleft", legend=c("Ahrens-Dieter (a<=1)", "GKM1 (a>1)"), col=c("red","green"),lty=1:2, cex=0.8)

```

We can see that both the number of iterations and the sample generation time are similar for both algorithms. Of course, there are some deviations, but they are not significant. This is very well shown in the above visualization, where we can see how, as expected, the number of iterations as well as the duration increases with the sample, but the values for both algorithms are close to each other. However, it can also be seen that for the Ahrens-Dieter algorithm the time increases faster than for the GKM1 algorithm.

## Modeling the insurance claims process
In order to model the insurance claims process, we will use the implemented generator from the Gamma distribution and a slightly modified generator from the Poisson distribution from the previous part of the project, i.e.

```{r claims-r9, echo=TRUE}
poisson <- function(N,theta){
    X <- c()
    while(length(X) < N){
      p <- exp(-theta)
      Fd <- p
      Nbig <- 0

      
      U <- runif(2)
      U1 <- U[2]
      
      while (U1 > Fd){
        Nbig <- Nbig + 1
        p <- p*theta/Nbig
        Fd <- Fd + p
        }
      X <- append(X,Nbig)
      }
    return(X)
}
```

In this chapter, we will model the insurance claims process by building a probabilistic model to describe the process of occurrence of aggregate insurance claims over a certain time period. Such an insurance system can be composed of a single policy or a group of insurance contracts. Aggregate claims are the sum of all claims served over a certain period of time. We can then distinguish between the individual risk model, which considers the loss from each individual contract, or the aggregate risk model, which models approaches that consider the distribution of frequency as well as severity of claims. It can be represented as 
$$S_N = X_1 + X_2 +... + X_N,$$
Where $N$ is a random variable denoting the number of losses (or the number of payouts). Naturally, we can assume $X_i>0$, because when $X_i = 0$ we have no claim. We assume that $N=n, X_1, X_2, ..., X_n$ are independent random variables with equal distributions. 

The distribution of $N$ is known as the frequency distribution. In our case, we will use the Poisson distribution, which is one of the known discrete distributions (besides, the binomial and negative binomial distributions are popular). As is known, we can consider this distribution as a distribution counting the number of occurrences of some random event in a certain unit of time, and therefore also counting the number of insurance claims in a given period.

In turn, the joint distribution of $X$ is known as the severity distribution. Since we approach the model of the insurance claims process through the collective risk model then we rely on separate modeling of the frequency and amount of claims. In such a situation, it is popular to use, as written above, frequency modeling through a Poisson process along with Gamma distribution modeling of claim amounts. In addition to this, a Pareto or Weibull distribution is also used. 

Our model algorithm is as follows:

 1. We assume a collective risk model $S_N = X_1 + .... + X_N$;
 
 2. let $j = 1, ..., m$ be an iterator set from $j=1$;
 
 3. We generate the number of claims $n_j$ from the frequency distribution $N$;
 
 4. Given a value of $n_j$, we generate the amount of each claim independently from the severity distribution $X$ denoted by $x_{1j}, ..., x_{n_jj}$;
 
 5. we recalculate the aggregate loss $s_j = x_{1j} + ... + x_{n_jj}$;
 
 6. we repeat the above three steps for $j=2, ..., m$ obtaining a sample $S_N$ of size $m$, i.e. ${s_1, ..., s_m}$. 

For our model, we consider the frequency distribution $N \sim Poiss(\lambda = 25)$ and the claim height distribution $X \sim Gamma(5, 2)$. The simulation code aggregated claim height is presented below.  

```{r claims-roszczeniemodel1, echo=TRUE}
insurance_model <- function(t, lambda=25, a=5, beta=2, seed=123){
  set.seed(seed)
  S <- rep(NA,t)
    
  n <- poisson(t, theta=lambda)
  for (j in 1:t){
    n_j <- n[j]
    x_j <- gam(a, beta, n_j, unlist=TRUE)
    s_j <- sum(x_j)
    S[j] <- s_j
  }
  return(S)
  
}
```

Let's see how it creates for the time $t = 12$ months with the given parameters.

```{r claims-checkmodelins, echo=TRUE}
S <- insurance_model(t=12)
hist(S)
print(paste("Total claims per 12-month interval: ", sum(S), sep=""))
```

We can see that the modeled sum of claims with these parameters is the corresponding amount in zlotys (let's assume for simplicity that our unit is the Polish zloty). In turn, from the histogram we can read the most common claim amounts. Let's check what is the probability, with these coefficients of frequency distributions and claim heights, that at time $t=12$ months the sum of claims will exceed $K=3000$ PLN. We will create a function to help estimate the probability for other sets of parameters. 
```{r claims-rroszczeniemodel, echo=TRUE}
insurance_model_probability <- function(K, t, N = 1000, lambda=25, a=5, beta=2){
  p = 0
  for (i in 1:N){
    seed = i
    
    S <- insurance_model(t = t, lambda = lambda, a = a, beta = beta, seed=seed)
    
    if (sum(S) > K){
      p = p+1
    }
  }
  
  prob = p/N

  print(paste("Probability of exceeding the sum K: ", K, " in time t: ", t, " equals: ", prob,       " for the parameters lambda= ", lambda, " ,a= ", a, " ,beta= ", beta, ".",sep=""))
  
  return(c(t, K, prob))
  }

```

```{r claims-rmodel4, echo=TRUE}
str1 <- insurance_model_probability(K = 3000,  t=12)
```

This means that, given the parameters, we are more likely to lose less than $3,000. Let's check how this probability changes on the range $[2950, 3350]PLN with a jump in $100 increments.

```{r claims-rmodelins6, echo=TRUE}
K <- c()
prob <- c()
for (ks in seq(2950,3350,100)){
    K <- append(K, ks)
    str1 <- insurance_model_probability(K = ks,  t=12)
    prob <- append(prob, str1[3])
}
plot(x= K, y=prob, type = "b", pch = 19, col="red", xlab= "Amount K", ylab = "Probability of exceeding K")
```

We can see that this probability decreases quite dramatically as the amount of $K$ increases. However, let's see a different arrangement of the parameters of the frequency and claim height distributions. Let's take $\lambda = $15$, $a = $9$ and $\beta = $5$. First, let's check the distribution of the sum of claims.

```{r claims-checkmodelins20, echo=TRUE}
S <- insurance_model(t=12, lambda = 15, a = 9, beta=5)
hist(S)
print(paste("Total claims per 12-month interval: ", sum(S), sep=""))
```

We can see that the total is much higher. From the histogram, we can again read the most common claims. Let's set, then, the check at these parameters for $K$ in the range $[8800,9200]$. 

```{r claims-rmodelins15, echo=TRUE}
K <- c()
prob <- c()
for (ks in seq(7800,8200,100)){
    K <- append(K, ks)
    str1 <- insurance_model_probability(K = ks,  t=12, lambda=15, a = 9, beta = 5)
    prob <- append(prob, str1[3])
}
plot(x= K, y=prob, type = "b", pch = 19, col="red", xlab= "Amount K", ylab = "Probability of exceeding K")
```

We see that here the probability is already falling at a much slower rate. Not surprisingly, in this case we expect an average frequency of 15 claims with a higher expected value which is due to the properties of Gamma and Poisson distributions. Using these two sets of parameters as an example, let's further examine the effect of the time period and change $t = 24$ months, a period of two years. First we will again look at the distributions of the sum of claims. For the first set of parameters: 

```{r claims-checkmodelins53, echo=TRUE}
S <- insurance_model(t=24)
hist(S, breaks = 15)
print(paste("Total claims per 24-month interval: ", sum(S), sep=""))
```

And for the second set:

```{r claims-checkmodelins76, echo=TRUE}
S <- insurance_model(t=24, lambda = 15, a = 9, beta=5)
hist(S, breaks = 15)
print(paste("Total claims per 24-month interval: ", sum(S), sep=""))
```

Naturally, the amount of claims has increased due to the extended number of possible time jumps, and the increase is nearly double as the period has increased. Let's see the effect on probability by setting $K$ on the range $[5900, 6300]$ and $[16400, 16800]$ for set 1 and 2, respectively. 

```{r claims-rmodelins15z1, echo=TRUE}
K <- c()
prob <- c()
for (ks in seq(5900,6300,100)){
    K <- append(K, ks)
    str1 <- insurance_model_probability(K = ks,  t=24)
    prob <- append(prob, str1[3])
}
plot(x= K, y=prob, type = "b", pch = 19, col="red", xlab= "Amount K", ylab = "Probability of exceeding K")
```

```{r claims-rmodelins15z2, echo=TRUE}
K <- c()
prob <- c()
for (ks in seq(16400,16800,100)){
    K <- append(K, ks)
    str1 <- insurance_model_probability(K = ks,  t=24, lambda=15, a = 9, beta = 5)
    prob <- append(prob, str1[3])
}
plot(x= K, y=prob, type = "b", pch = 19, col="red", xlab= "Amount K", ylab = "Probability of exceeding K")
```

The conclusions are analogous. Because of the choice of parameters, set 1 has a faster decreasing probability of exceeding a certain amount $K$ in the time interval $[0,t]$, while for set 2 this probability decreases more slowly. Increasing the time interval, therefore, does not "stabilize" our probability of exceeding a certain level of loss. In addition, a seed was set in each simulation to reproduce the results, but the study was previously conducted without this treatment. The results differed in each simulation by sizable amounts, which indicates the high unpredictability of the amount of claims. Obviously, here the parameters were chosen "in the dark," but it is the appropriate choice of distribution parameters that poses the challenge for correct modeling of the phenomenon.  


## Summary
In the next project, two generators of numbers from the Gamma distribution were implemented along with a theoretical description. Each of them considered a different case of the parameter $a$, so they were combined into one generator depending on the value of this parameter. Moreover, samples were generated from them and visualized by comparing them to the built-in generators. The results were satisfactory.

In the next part, the previously built generator was transformed from the Poisson distribution in terms of creating a model of the insurance claims process. The sum of loss (insurance claim volume) was considered using a collective risk model using Poisson distributions for frequency modeling and Gamma distributions for claim amount modeling. An example was then considered for different parameter arrangements and for each of them the probability of a loss above a fixed level $K$ was estimated for a certain time interval $[0,t]$. These results were presented in graphical form and subjected to interpretation. 





